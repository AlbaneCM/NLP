{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d4037f",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a2e48",
   "metadata": {},
   "source": [
    "**Author**: Albane Colmenares <br>\n",
    "**Date**: December 12th, 2023 <br>\n",
    "___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e25ce2",
   "metadata": {},
   "source": [
    "### <u>Table of Content</u>\n",
    "**1. Overview**<br>\n",
    "**2. Business Understanding**<br>\n",
    "**3. Data Understanding**<br>\n",
    "**4. Data Preparation**<br>\n",
    "**5. Modeling**<br>\n",
    "**6. Evaluation**<br>\n",
    "**7. Findings & Recommendations**<br>\n",
    "**8. Limits & Next Steps**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bc745",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88bbf7",
   "metadata": {},
   "source": [
    "This notebook examines tweets about several brands and products and predicts whether the sentiment of the short text is positive, negative or neutral. <br>\n",
    "The organization of this notebook follows the CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model that serves as the base for a data science process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9197d",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc41608",
   "metadata": {},
   "source": [
    "## 2. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5355c",
   "metadata": {},
   "source": [
    "Business and data understanding: *what kind of data are you using, and what makes it well-suited for the business problem?*\n",
    "* You do not need to include any data visualizations in your summary, but consider including relevant descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38a169",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868d126",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0faaa",
   "metadata": {},
   "source": [
    "## 3. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8bc5af",
   "metadata": {},
   "source": [
    "**Data Source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e4ef7",
   "metadata": {},
   "source": [
    "The data comes from CrowdFlower via [data.world](https://data.world/crowdflower/brands-and-product-emotions). \n",
    "\n",
    "\n",
    "------------- REPHRASE THIS ------------- \n",
    "\n",
    "*Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.*\n",
    "\n",
    "------------- REPHRASE THIS ------------- \n",
    "\n",
    "The file `judge-1377884607_tweet_product_company.csv` can be downloaded at the provided link. \n",
    "It was then renamed to `tweet_product_company.csv`and saved into the current folder, within the 'data' subfolder, to be accessed into the raw DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56079d",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e546fea",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e3b22",
   "metadata": {},
   "source": [
    "Prior to preprocessing, the columns are: \n",
    "\n",
    "* `tweet_text`: the actual tweet's record\n",
    "* `emotion_in_tweet_is_directed_at`: the product or company referred to in the tweet\n",
    "* `is_there_an_emotion_directed_at_a_brand_or_product`: the tweet's sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a7f11",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402a218",
   "metadata": {},
   "source": [
    "**Target**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0413b34",
   "metadata": {},
   "source": [
    "The tweet's sentiment is the target for the dataset. The specific column is `is_there_an_emotion_directed_at_a_brand_or_product`. Based on a given set of tweets, we will try to predict if the tweet's emotion was positive, negative or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7405f",
   "metadata": {},
   "source": [
    "**Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1f915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79d550d",
   "metadata": {},
   "source": [
    "The text file is encoded using Latin-1 encoding - and is open as is. Several encodings were tried to ensure the right one matched: utf-8, utf-16, ascii for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fa494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset and saving it as raw_df\n",
    "raw_df = pd.read_csv('data/tweet_product_company.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847e810",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspecting the first 5 rows of the DataFrame\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The dataset has '+ str(len(raw_df)) + ' rows and 3 columns.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12216ee6",
   "metadata": {},
   "source": [
    "The various companies and products referred to in the tweets will be reviewed to get an understand of the balance in the dataset, along with what is being most often reviewed.  \n",
    "\n",
    "Similarly, the emotions will be reviewed in a similar way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4e4c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the number of tweets referring to each product or company\n",
    "raw_df['emotion_in_tweet_is_directed_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe7717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the number of tweets referring to each emotion\n",
    "raw_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbb6ec",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be807149",
   "metadata": {},
   "source": [
    "This includes data cleaning and exploratory data analysis with `nltk`\n",
    "\n",
    "\n",
    "more text <br>\n",
    "more text <br>\n",
    "more text <br>\n",
    "more text <br>\n",
    "more text <br>\n",
    "more text <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4da52",
   "metadata": {},
   "source": [
    "*why did you choose the data preparation steps that you did, and what was the result?*\n",
    "\n",
    "* This should be specific to the kind of data you are working with. For example, if you are doing an NLP project, what did you decide to do with stopwords?\n",
    "* Be sure to list the packages/libraries used to prepare the data, and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b857f",
   "metadata": {},
   "source": [
    "For a better readability of the tweets' texts, the column width will be increased. In addition, the use of MathJax will be disabled so that the visual representation of mathematical expressions are not displayed so this doesn't cause issues to the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing column width\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784b844",
   "metadata": {},
   "source": [
    "### 4. a) Column names' change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3497778",
   "metadata": {},
   "source": [
    "The column names are particularly long. For an easier process to handle, they will be renamed in the new DataFrame called `df`:\n",
    "* `tweet`\n",
    "* `product_or_company`\n",
    "* `sentiment`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the raw DataFrame to modify it\n",
    "df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the new columns' names and attributing them to the new DataFrame\n",
    "df.columns = ['tweet', 'product_or_company', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac7e5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the changes applied  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4af38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c4d20cc",
   "metadata": {},
   "source": [
    "### 4. c) Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c57da",
   "metadata": {},
   "source": [
    "In the next section, the missing values are inspected and handled by category. \n",
    "<br>\n",
    "The `tweet` column only had 1 row with null values and had no implication on other features: it is removed. \n",
    "<br>\n",
    "The `product_or_company` requires contains many more missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4490695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897d890",
   "metadata": {},
   "source": [
    "* **Tweet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1008ea2",
   "metadata": {},
   "source": [
    "The tweet column only has one null value with no information on the other columns: it will be dropped from the DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe1f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the tweet containing null information \n",
    "df[df['tweet'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c51618",
   "metadata": {},
   "source": [
    "The null tweet does not contain any information for either column and will be dropped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null tweet from the DataFrame\n",
    "\n",
    "df = df.dropna(subset=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1401ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying it was correctly removed\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b70ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The dataset now has '+ str(len(df)) + '. The missing tweet was removed.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e13bd",
   "metadata": {},
   "source": [
    "* **Product or Company**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acca6ca",
   "metadata": {},
   "source": [
    "The product_or_company column contains many null values where neither the product or the brand was specified. For now, all null values will be replaced by 'unknown', as the focus is to predict sentiment. \n",
    "<br>If the focus on product or company needs to be done, two columns will be created to identify the product and the brand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894c971",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspecting the tweet containing null information \n",
    "df[df['product_or_company'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the null product or company with 'undefined'\n",
    "df['product_or_company'] = df['product_or_company'].fillna('undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222f880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying it was correctly handled\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The dataset still has '+ str(len(df)) + '.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed26587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the count of rows by unique value in this column\n",
    "df['product_or_company'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfb9ad",
   "metadata": {},
   "source": [
    "### 4. d) Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows were duplicates\n",
    "print(str(len(df[df.duplicated()])) + f' duplicate rows were identified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79fdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the duplicate rows\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying with one example that tweets were indeed duplicated \n",
    "df[df['tweet'] == 'Before It Even Begins, Apple Wins #SXSW {link}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a598d6",
   "metadata": {},
   "source": [
    "### 4. d) Turning sentiment classification into a binary one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328881f",
   "metadata": {},
   "source": [
    "* **Product or Company**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b1f74",
   "metadata": {},
   "source": [
    "The product or company column does not have an impact on whether a tweet is positive or negative, so it will not be transformed as it will not be used further for predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70baaac",
   "metadata": {},
   "source": [
    "* **Sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee879e",
   "metadata": {},
   "source": [
    "Four sentiment categories are described, which could be grouped in three: positive, negative, neutral. \n",
    "<br>This is what will be covered over the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0163c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows by emotion\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533c0b3",
   "metadata": {},
   "source": [
    "* **Categorizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790dc9b",
   "metadata": {},
   "source": [
    "Due to the nature of the target, we will focus on the positive ones. Hence all the other tweets, whether they are neutral or negative, will be considered *not positive*, so will be identified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the new classifications for the sentiment column \n",
    "classification_columns = {\n",
    "    'sentiment': {\n",
    "        \"No emotion toward brand or product\": \"negative\", \n",
    "        \"I can't tell\": \"negative\", \n",
    "        \"Positive emotion\": \"positive\", \n",
    "        \"Negative emotion\": \"negative\" \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the sentiment column classification\n",
    "\n",
    "# Defining columns to change\n",
    "column_classification = ['sentiment']\n",
    "\n",
    "def convert_class(df, columns_mapping):\n",
    "    for column, mapping in columns_mapping.items():\n",
    "        print('Before: ' + column, df[column].unique())\n",
    "        df[column] = df[column].map(mapping)\n",
    "        print('After: ' + column, df[column].unique())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cfc7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_class(df, classification_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows by unique sentiment\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bar chart for to visualize class imbalance\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Defining custom colors \n",
    "custom_colors = ['#3B3935', '#00917C']\n",
    "\n",
    "sns.countplot(data=df, x='sentiment', order=df['sentiment'].value_counts().index, palette=custom_colors)\n",
    "\n",
    "ax.set_xlabel(xlabel = 'Sentiment', fontsize=15)\n",
    "ax.set_ylabel(ylabel = 'Number of Tweets', fontsize=15)\n",
    "\n",
    "ax.set_xticklabels(labels=['Negative', 'Positive'])\n",
    "\n",
    "ax.set_title(f'Number of tweets per sentiment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc6807",
   "metadata": {},
   "source": [
    "### 4. e) Performing a Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df into X and y\n",
    "X = df.drop('sentiment', axis=1)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, _test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7863e",
   "metadata": {},
   "source": [
    "* **Distribution of Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = pd.DataFrame(y_train.value_counts())\n",
    "train_target_counts.index.name = 'target name'\n",
    "train_target_counts.rename(columns={'sentiment': 'count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19ce95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_target_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92d494",
   "metadata": {},
   "source": [
    "* **Visually Inspecting Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee42837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a sample of 5 records to display the full text of each\n",
    "train_sample = X_train.sample(5, random_state=22)\n",
    "train_sample['label'] = [y_train[val] for val in train_sample.index]\n",
    "train_sample.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fa800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d2cdc9",
   "metadata": {},
   "source": [
    "## 4. or 5. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6f72c",
   "metadata": {},
   "source": [
    "## 4. Data Preparation Continuity or Preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb1807",
   "metadata": {},
   "source": [
    "### 4. e) Standardizing Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0854ff5",
   "metadata": {},
   "source": [
    "Before starting any exploratory analysis, two fundamental data cleaning tasks will be performed on the text data: standardizing case and tokenizing. The first one will be standardizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9777a",
   "metadata": {},
   "source": [
    "We will glance at the first sample of tweet to get an idea of whether we need to standardize case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating the first tweet into windows_sample\n",
    "windows_sample = train_sample.iloc[0][\"tweet\"]\n",
    "windows_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a3d9c",
   "metadata": {},
   "source": [
    "Changing to lower case is necessary. We will apply this to the first tweet sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming sample data to lowercase\n",
    "windows_sample.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe71430",
   "metadata": {},
   "source": [
    "This answers our needs - we will apply this to our sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8f99e",
   "metadata": {},
   "source": [
    "* **Lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dae3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming sample data to lowercase\n",
    "train_sample['tweet'] = train_sample['tweet'].str.lower()\n",
    "# Displaying full text\n",
    "train_sample.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492fe78",
   "metadata": {},
   "source": [
    "This answers our needs - we will apply this to our full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b024d76",
   "metadata": {},
   "source": [
    "* **Standardizing Case in the Full Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cadd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming full training data to lowercase\n",
    "X_train['tweet'] = X_train['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23eb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying an example to see if this applied correctly\n",
    "X_train.iloc[100]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc305a",
   "metadata": {},
   "source": [
    "### 4. f) Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab50da4",
   "metadata": {},
   "source": [
    "The second fundamental data cleaning step is to tokenize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing one of our train_sample tweets\n",
    "tweet_sample = train_sample.iloc[1]['tweet']\n",
    "tweet_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c308e",
   "metadata": {},
   "source": [
    "We will use `RegexpTokenizer` from NLTK to create tokens of tow or more consecutive word characters, which include letters, numbers and underscores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aefe0d",
   "metadata": {},
   "source": [
    "* **Tokenizing Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "tokenizer.tokenize(tweet_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33114691",
   "metadata": {},
   "source": [
    "* **Tokenizing the Full Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27339ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column tweet_tokenized on X_train\n",
    "X_train['tweet_tokenized'] = X_train['tweet'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting a tweet example\n",
    "X_train.iloc[99][['tweet', 'tweet_tokenized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f705e0",
   "metadata": {},
   "source": [
    "We have removed all single-letter words, so instead of \"i\", \"got\", \"in\". We now have'got', 'in'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f739b",
   "metadata": {},
   "source": [
    "## ?. Exploratory Data Analysis: Frequency Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b076ab",
   "metadata": {},
   "source": [
    "A frequency distribution is a data structure that contains pieces of data as well as the count of how frequently they appear. \n",
    "In this case, pieces of data are words. \n",
    "\n",
    "In order to do this, we will use the `FreqDist` package, which allows us to pass in a single list of words, and produces a dictionary-like output of those words and their frequencies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13a95f",
   "metadata": {},
   "source": [
    "We will visualize the top 10 words to evaluate further what cleaning needs to be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant package: FreqDist\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e68c3",
   "metadata": {},
   "source": [
    "* **FreqDist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f47008",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_freq_dist = FreqDist(X_train.iloc[100]['tweet_tokenized'][:20])\n",
    "example_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant package for top number of words\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Creating a function to visualize the top 10 words\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "#     extracting data for graph\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "    \n",
    "#     Setting up graph and plotting data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "    \n",
    "#     Custominzing plot appearance \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    \n",
    "visualize_top_10(example_freq_dist, \"Top 10 Word Frequency for Example Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b33b2c4e",
   "metadata": {},
   "source": [
    "* **FreqDist on the Full DataSet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614121d1",
   "metadata": {},
   "source": [
    "In order to calculate the count of words, they need to be stored into a list. To do so, we will `explode` the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8269410",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a frequency distribution for X_train\n",
    "train_freq_dist = FreqDist(X_train['tweet_tokenized'].explode())\n",
    "\n",
    "# Plotting the top 10 tokens\n",
    "visualize_top_10(train_freq_dist, 'Top 10 Word Frequency for Full X_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5eb279",
   "metadata": {},
   "source": [
    "We can also subdivide this by category to see if it makes a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in labels for filtering\n",
    "X_train['label'] = [y_train[val] for val in X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining funcrion to plot 2 visualizations\n",
    "\n",
    "# Creating two columns \n",
    "def two_subplits():\n",
    "    fig = plt.figure(figsize=(15, 9))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0]) #row 0, col 0 \n",
    "    ax2 = fig.add_subplot(gs[0, 1]) #row 0, col 1 \n",
    "    return fig, [ax1, ax2]\n",
    "\n",
    "# Plotting the graph\n",
    "def plot_distribution_by_sentiment(X_version, column, axes, title = \"Word Frequency for:\"):\n",
    "    for index, category in enumerate(X_version['label'].unique()): \n",
    "#         Calculating frequency distribution for this subset\n",
    "        all_words = X_version[X_version['label'] == category][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "        \n",
    "        \n",
    "#         Setting up a plot\n",
    "        ax = axes[index]\n",
    "        ax.bar(tokens, counts)\n",
    "        \n",
    "#         Customizing plot appearance\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "        \n",
    "        \n",
    "fig, axes = two_subplits()\n",
    "plot_distribution_by_sentiment(X_train, 'tweet_tokenized', axes)\n",
    "fig.suptitle('Word Frequencies for Each Sentiment', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4e63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61f7b24a",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e686b3c",
   "metadata": {},
   "source": [
    "*what modeling package(s) did you use, which model(s) within the package(s), and what tuning steps did you take?*\n",
    "* For some projects there may be only one applicable package; you should still briefly explain why this was the appropriate choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9842e",
   "metadata": {},
   "source": [
    "### 5. a) Baseline Model with TfidfVectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564c01a",
   "metadata": {},
   "source": [
    "We will start modeling by building an initial model which only has access to the information in the plots above. So, using the default token pattern to split the full text into tokens, and using a limited vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1406ff",
   "metadata": {},
   "source": [
    "To give the model a little bit more information with those same features, `TfidVectorizer` will be used to count the term frequency (`tf`) within a single document. This package also includes the inverse document frequency (`idf`): how rare the term is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96e24d",
   "metadata": {},
   "source": [
    "The first step is to import the vectorizer, instantiate a vectorizer object and fit it on `X_train['tweet']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c95417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiating a vectorizer with max_features=10 \n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "\n",
    "# Fitting the vectorizer on X_train['tweet'] and transforming it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['tweet'])\n",
    "\n",
    "# Inspecting the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a86f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570eeb6b",
   "metadata": {},
   "source": [
    "Now that we have preprocessed data, we will fit and evaluate Naive Bayes classifier using `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant class function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiating a MultinomialNB classifier\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "# Evaluating the classifier on X_train_vectorized and y_train\n",
    "# Since we are trying to measure the positive sentiment, we need to subtract the cross val score from 1: \n",
    "# positive is the second sentiment\n",
    "baseline_cv = 1 - cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7d64b",
   "metadata": {},
   "source": [
    "**Verifying the class balance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cefe6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the class balance\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb96739",
   "metadata": {},
   "source": [
    "How well did the final model perform?\n",
    "\n",
    "If we guessed the contribution of sentiment every time, we would expect about 33% accuracy. \n",
    "Our model baseline is not getting more than just getting every time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73438fae",
   "metadata": {},
   "source": [
    "-----------------------ADD THE OTHER EVALUATION METRICS-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49bdbd",
   "metadata": {},
   "source": [
    "### <u>2nd iteration</u>: Addressing class imbalance: undersampling negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the undersampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "# Applying undersampling only on training data\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1a4bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting the vectorizer on X_resampled['tweet'] and transforming it\n",
    "X_resampled_vectorized = tfidf.fit_transform(X_resampled[\"tweet\"])\n",
    "\n",
    "# Inspecting the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef71e5",
   "metadata": {},
   "source": [
    "Now that we have preprocessed data, we will fit and evaluate Naive Bayes classifier using `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c415ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the classifier on X_train_vectorized and y_train\n",
    "balanced_cv = 1- cross_val_score(baseline_model, X_resampled_vectorized, y_resampled)\n",
    "balanced_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c0aa7",
   "metadata": {},
   "source": [
    "The cross_val_score considerably improved from 33% to 44%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the new class balance\n",
    "y_resampled.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d96ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdfd9c4a",
   "metadata": {},
   "source": [
    "### <u>3rd iteration</u>: Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028dd18",
   "metadata": {},
   "source": [
    "**Removing Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283d8fc",
   "metadata": {},
   "source": [
    "Typical list of stopwords to which we will add:\n",
    "* `sxsw`: the name of the conference \n",
    "* `mention`: when tweeted\n",
    "* `link`: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Creating list to store stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a710a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing words to add to list of stopwords\n",
    "manual_stopwords = ['sxsw', 'mention', 'link', 'rt']\n",
    "\n",
    "# Adding to list of stopwords\n",
    "for word in manual_stopwords:\n",
    "    stopwords_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf269f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the new words were added\n",
    "stopwords_list[-len(manual_stopwords):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case some stopwords need to be removed\n",
    "# stopwords_list.remove(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a98b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a list of strings and returns only those that are not in the list\n",
    "def remove_stopwords(token_list):\n",
    "    stopwords_removed = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d27fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing it on an example\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7acfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_example = X_train.iloc[100]['tweet_tokenized']\n",
    "print(\"Length with stopwords: \", len(tokens_example))\n",
    "\n",
    "tokens_example_without_stopwords = remove_stopwords(tokens_example)\n",
    "print(\"Length with stopwords: \", len(tokens_example_without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151471a",
   "metadata": {},
   "source": [
    "Applying it to all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled['tweet_tokenized_without_stopwords'] = X_resampled['tweet_tokenized'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02093278",
   "metadata": {},
   "source": [
    "Now let's compare the frequency distribution without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20910b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = two_subplits()\n",
    "plot_distribution_by_sentiment(X_resampled, 'tweet_tokenized_without_stopwords', axes)\n",
    "fig.suptitle('Word Frequencies for Each Sentiment', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train_vectorized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db552e0e",
   "metadata": {},
   "source": [
    "We will now re-run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d146433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the new vectorizer \n",
    "tfidf = TfidfVectorizer(\n",
    "        max_features=10,\n",
    "        stop_words=stopwords_list\n",
    "        )\n",
    "\n",
    "# Fitting the vectorizer on X_resampled['tweet'] and transforming it\n",
    "X_resampled_vectorized = tfidf.fit_transform(X_resampled['tweet'])\n",
    "\n",
    "\n",
    "# Visually inspecting the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_resampled_vectorized, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaacf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the classifier on X_train_vectorized and y_resampled\n",
    "stopwords_removed_cv = 1- cross_val_score(baseline_model, X_resampled_vectorized, y_resampled)\n",
    "stopwords_removed_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b637bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Baseline:         \", baseline_cv.mean())\n",
    "print(\"Balanced:         \", balanced_cv.mean())\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c9b01",
   "metadata": {},
   "source": [
    "This is an improvement but a lower accuracy than prior to stopwords being removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2bebdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23803a96",
   "metadata": {},
   "source": [
    "### <u>3rd bis</u>: Stopwords should not be removed, only some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796c2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ba3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ce920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ddb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45aacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490ef92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497eaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1664f5bd",
   "metadata": {},
   "source": [
    "### <u>4th iteration</u>: Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96882b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant package\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Instantiating the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997021c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Lemmatizer\n",
    "def lemmatize_words(token_list):\n",
    "    lemmatized_token = [lemmatizer.lemmatize(token, pos='v') for token in token_list]\n",
    "    return lemmatized_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2401357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our tokens_examples\n",
    "tokens_example = X_resampled.iloc[300]['tweet_tokenized_without_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_words(tokens_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48489914",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5931c29",
   "metadata": {},
   "source": [
    "*how well did your final model perform?*\n",
    "* Include one or more relevant metrics\n",
    " \n",
    "* Be sure to briefly describe your validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570161ef",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d025f",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60006f49",
   "metadata": {},
   "source": [
    "## 7. Findings & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b153038",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bb239",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ade41",
   "metadata": {},
   "source": [
    "## 8. Limits & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8065e3",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb2dd8",
   "metadata": {},
   "source": [
    "Text Text Text Text Text Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
